# Ensemble

한 데이터셋을 쪼개서 학습 여러 번 하는 방식

종류: Bagging (Bootstrap Aggregation), Boosting, RandomForest

## 1. Bagging

샘플을 여러 개로 나눠서 따로 학습 → 마지막에 평균을 내는 구조

- 병렬적 학습 방식
- **boostrap**: 데이터를 쪼개서 샘플링한다는 뜻
- 대표적인 방식 : **RandomForest**
- 주로 과적합에 쓰임

## 2. Boosting

이전 학습에서 잘못 분류한 것에 더 큰 가중치를 부여한 후 학습

- 순차적 학습 방식
- 단점 : 과대적합에 취약함
- 종류 : AdaBoost, Gradient Boosting, XGBoost
- 주로 정확도를 높일 때 쓰임

---

# 비지도 학습

정답 지표가 없는 데이터의 패턴을 찾는 방식 (라벨링이 필요없음)

## 1. Clustering

군집화

- **K-Means** : k값을 지정하여 몇 개의 군집을 묶을지 설정한다. 대표적으로 엘보우 기법을 사용한다. 중심점과의 거리는 주로 유클리디안 거리로 측정해 군집을 생성한다.
- **Hierarchical Clustering** : 거리를 계산한 후 클러스터링. 단일 연결, 완전 연결, 평균 연결, ward 거리 등등의 방식으로 측정한 거리를 병합한다. 이 과정을 반복한다.
- **DBSCAN** : 점이 얼마나 몰려있는가 측정한다. 이 밀도가 높은 지역은 하나의 군집으로 설정하고, 밀도가 낮은 지역은 noise로 처리한다. 몇개의 점을 기준으로 할지 설정할 수 있다.

## 2. 차원 축소

고차원 데이터를 저차원으로 바꿔서 처리 효율을 향상시키는 방식(실행시간이 단축됨)

- **PCA**: 데이터를 잘 나타내는 최적의 직선을 찾고, 이에 따라 차원을 축소한다. 어느 정도의 데이터 손실은 감수한다. 선형적인 방식이다.
- **t-SNE** : 비선형적인 차원 축소 방식이다. 고차원에서 사용한다.